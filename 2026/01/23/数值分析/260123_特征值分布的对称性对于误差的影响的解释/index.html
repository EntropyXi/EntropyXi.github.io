<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  
  <title>特征值分布的对称性对于误差的影响的解释 | Hexo</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta name="description" content="我们设I是一个100 × 100单位矩阵，Z是一个100 × 100的零矩阵，b是一个200 × 1的全1向量 设$\mathbf{B}&#x3D;\begin{bmatrix} 1 &amp; &amp; &amp; \\ &amp; 2 &amp; &amp; \\ &amp; &amp; \ddots &amp; \\ &amp; &amp; &amp; 100 \end{bmatrix}$ 然后">
<meta property="og:type" content="article">
<meta property="og:title" content="特征值分布的对称性对于误差的影响的解释">
<meta property="og:url" content="http://example.com/2026/01/23/%E6%95%B0%E5%80%BC%E5%88%86%E6%9E%90/260123_%E7%89%B9%E5%BE%81%E5%80%BC%E5%88%86%E5%B8%83%E7%9A%84%E5%AF%B9%E7%A7%B0%E6%80%A7%E5%AF%B9%E4%BA%8E%E8%AF%AF%E5%B7%AE%E7%9A%84%E5%BD%B1%E5%93%8D%E7%9A%84%E8%A7%A3%E9%87%8A/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="我们设I是一个100 × 100单位矩阵，Z是一个100 × 100的零矩阵，b是一个200 × 1的全1向量 设$\mathbf{B}&#x3D;\begin{bmatrix} 1 &amp; &amp; &amp; \\ &amp; 2 &amp; &amp; \\ &amp; &amp; \ddots &amp; \\ &amp; &amp; &amp; 100 \end{bmatrix}$ 然后">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2026-01-23T04:46:14.000Z">
<meta property="article:modified_time" content="2026-02-08T06:27:55.112Z">
<meta property="article:author" content="John Doe">
<meta property="article:tag" content="数值分析">
<meta name="twitter:card" content="summary">
  
    <link rel="alternate" href="/atom.xml" title="Hexo" type="application/atom+xml">
  
  
    <link rel="shortcut icon" href="/favicon.png">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    
<link rel="stylesheet" href="/fancybox/jquery.fancybox.min.css">

  
  
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/fork-awesome@1.2.0/css/fork-awesome.min.css">

<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">
<!-- hexo injector head_end end --><meta name="generator" content="Hexo 8.1.1"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Hexo</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"><span class="fa fa-bars"></span></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
        
          <a class="nav-icon" href="/atom.xml" title="RSS Feed"><span class="fa fa-rss"></span></a>
        
        <a class="nav-icon nav-search-btn" title="Search"><span class="fa fa-search"></span></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://example.com"></form>
      </div>
    </div>
  </div>
</header>

      <div class="outer">
        <section id="main"><article id="post-数值分析/260123_特征值分布的对称性对于误差的影响的解释" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2026/01/23/%E6%95%B0%E5%80%BC%E5%88%86%E6%9E%90/260123_%E7%89%B9%E5%BE%81%E5%80%BC%E5%88%86%E5%B8%83%E7%9A%84%E5%AF%B9%E7%A7%B0%E6%80%A7%E5%AF%B9%E4%BA%8E%E8%AF%AF%E5%B7%AE%E7%9A%84%E5%BD%B1%E5%93%8D%E7%9A%84%E8%A7%A3%E9%87%8A/" class="article-date">
  <time class="dt-published" datetime="2026-01-23T04:46:14.000Z" itemprop="datePublished">2026-01-23</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="p-name article-title" itemprop="headline name">
      特征值分布的对称性对于误差的影响的解释
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <p>我们设<span class="math inline"><strong>I</strong></span>是一个<span
class="math inline">100 × 100</span>单位矩阵，<span
class="math inline"><strong>Z</strong></span>是一个<span
class="math inline">100 × 100</span>的零矩阵，<span
class="math inline"><strong>b</strong></span>是一个<span
class="math inline">200 × 1</span>的全1向量 设<span
class="math inline">$\mathbf{B}=\begin{bmatrix} 1 &amp; &amp; &amp; \\
&amp; 2 &amp; &amp; \\ &amp; &amp; \ddots &amp; \\ &amp; &amp; &amp; 100
\end{bmatrix}$</span> 然后，令<span
class="math inline">$\mathbf{A_1}=\begin{bmatrix} \mathbf{B} &amp;
\mathbf{I} \\ \mathbf{Z} &amp; \mathbf{B} \end{bmatrix}$</span> ，<span
class="math inline">$\mathbf{A_2}=\begin{bmatrix} \mathbf{B} &amp;
\mathbf{I} \\ \mathbf{Z} &amp; \mathbf{-B} \end{bmatrix}$</span>
我们使用GMRES算法，分别绘制<span
class="math inline"><strong>A</strong><sub><strong>1</strong></sub></span>与<span
class="math inline"><strong>A</strong><sub><strong>2</strong></sub></span>的残差收敛图，如下图所示
![[特征值分布的对称性对于误差的影响的解释.png]]</p>
<p>我们关注两个问题 #### 1.为什么<span
class="math inline"><em>A</em><sub>1</sub></span>的收敛曲线是光滑的，而<span
class="math inline"><em>A</em><sub>2</sub></span>的是带有折线的？ ####
2.为什么<span
class="math inline"><em>A</em><sub>1</sub></span>收敛得更快，而<span
class="math inline"><em>A</em><sub>2</sub></span>收敛极其缓慢？</p>
<p>我们先来回顾一下Arnoldi迭代 <strong>Arnoldi
迭代本质上是在构造一组正交多项式</strong> 我们生成的基向量<span
class="math inline"><em>q</em><sub>1</sub>, <em>q</em><sub>2</sub>, <em>q</em><sub>3</sub>…</span>，其实他们代表了<span
class="math inline"><strong>A</strong></span>的不同次幂作用在<span
class="math inline"><strong>b</strong></span>上 - <span
class="math inline"><em>q</em><sub>1</sub></span>：它是<span
class="math inline"><strong>b</strong></span>的归一化
本质：零次多项式<span
class="math inline"><em>ϕ</em><sub>0</sub>(<em>A</em>)<em>b</em></span>
- <span class="math inline"><em>q</em><sub>2</sub></span>：来自<span
class="math inline"><em>A</em><em>q</em><sub>1</sub></span>（并减去了<span
class="math inline"><em>q</em><sub>1</sub></span>分量） 本质：包含<span
class="math inline"><em>A</em><em>b</em></span>和<span
class="math inline"><em>b</em></span>，即1次多项式<span
class="math inline"><em>ϕ</em><sub>1</sub>(<em>A</em>)<em>b</em></span>
- <span class="math inline"><em>q</em><sub>3</sub></span>：来自<span
class="math inline"><em>A</em><em>q</em><sub>2</sub></span>
本质：2次多项式<span
class="math inline"><em>ϕ</em><sub>2</sub>(<em>A</em>)<em>b</em></span>
这意味着，你的矩阵<span
class="math inline"><em>Q</em><sub><em>m</em></sub></span>的每一列，本质上都是一个以向量形式存在的，作用在<span
class="math inline"><em>b</em></span>上的多项式</p>
<p>然后，GMRES算法本质上是在求解一个最小二乘问题，也就是我们通过了这个算法得到了一个用来”形容“<span
class="math inline"><em>z</em></span>的最优多项式，</p>
<p><span
class="math display"><em>m</em><em>i</em><em>n</em>||<em>H</em><sub><em>m</em></sub><em>z</em> − ||<em>b</em>||<em>e</em><sub>1</sub>||</span></p>
<p>我们会得到一个解向量<span
class="math inline"><strong>y</strong> = [<em>y</em><sub>1</sub>, <em>y</em><sub>2</sub>, …, <em>y</em><sub><em>m</em></sub>]<sup><em>T</em></sup></span>（就是<span
class="math inline"><em>z</em></span>）
这是配方，它告诉我们对于多项式的每一项，我们应该取多少项的<span
class="math inline"><em>q</em><sub><em>m</em></sub></span> 因为<span
class="math inline"><em>y</em></span>是让残差最小的“最优配方”，所以它组合出来的多项式，自然就是最优多项式</p>
<p>而我们去寻找这个最优多项式，要去令残差<span
class="math inline"><strong>r</strong> = <em>p</em>(<em>A</em>)<strong>b</strong></span>最小，本质上也是去<strong>把多项式中特征值的位置（大小）压低。</strong>
这是什么意思？ 我们知道，在矩阵<span
class="math inline"><em>A</em></span>张成的空间里，任何一个向量<span
class="math inline"><em>b</em></span>都可以写成这些特征向量的叠加：</p>
<p><span
class="math display"><strong>b</strong> = <em>y</em><sub>1</sub><strong>v</strong><sub><strong>1</strong></sub> + <em>y</em><sub>2</sub><strong>v</strong><sub><strong>2</strong></sub> + … + <em>y</em><sub><em>n</em></sub><strong>v</strong><sub><strong>n</strong></sub></span></p>
<p>又有<span
class="math inline"><em>p</em>(<em>A</em>)<strong>v</strong><sub><strong>i</strong></sub> = <em>p</em>(<em>λ</em><sub><em>i</em></sub>)<strong>v</strong><sub><strong>i</strong></sub></span>，所以，残差<span
class="math inline"><strong>r</strong> = <em>p</em>(<em>A</em>)<strong>b</strong></span>就可以表示为</p>
<p><span class="math display">$$
\begin{aligned}
\mathbf{r} &amp;= p(A)\mathbf{b} \\
           &amp;= p(A)(y_1\mathbf{y_1}+\dots+y_n\mathbf{v_n}) \\
           &amp;=
y_1p(\lambda_1)\mathbf{v_1}+\dots+y_np(\lambda_n)\mathbf{v_n}
\end{aligned}
$$</span></p>
<p>所以</p>
<p><span
class="math display">||<strong>r</strong>|| ≈ ∑||<em>c</em><sub><em>i</em></sub><em>p</em>(<em>λ</em><sub><em>i</em></sub>)<em>v</em><sub><em>i</em></sub>||</span></p>
<p>我们会发现，如果我们要让残差最小，我们得让<span
class="math inline"><em>p</em>(<em>λ</em>)</span>最小，如果你想让残差逼近0，那你就得让<span
class="math inline"><em>p</em>(<em>λ</em>) = 0</span></p>
<p>现在准备工作准备得差不多，我们可以开始看问题了</p>
<ul>
<li>首先，<strong>为什么<span
class="math inline"><em>A</em><sub>2</sub></span>的收敛图像会带有折线？</strong>
观察<span
class="math inline"><em>A</em><sub>2</sub></span>的结构，它是一个稀疏矩阵且对角线上的元素值是关于原点对称的，所以我们可以近似认为，这个矩阵的特征值是关于原点对称的。
我们注意到，奇数步时多项式的最高次项是奇数项
<ul>
<li>而奇次函数是反对称的：<span
class="math inline"><em>f</em>(<em>λ</em>) = −<em>f</em>(−<em>λ</em>)</span></li>
<li>这就意味这，如果你想去用奇次项去压低正特征值处的误差，负特征值处的误差就会被放大了。例如，现在我们有一个奇次函数<span
class="math inline"><em>f</em>(<em>x</em>) = 1 + <em>c</em><em>x</em></span>，矩阵有特征值1和-1，我们先令<span
class="math inline"><em>f</em>(1) ≈ 0</span>，即<span
class="math inline">1 + <em>c</em> · (1) ≈ 0</span>，我们取理想值<span
class="math inline"><em>c</em> = −1</span>，此时<span
class="math inline"><em>p</em>(1) = 1 + (−1) · 1 = 0</span>，正特征值处的误差被消除了。那我们再看<span
class="math inline"><em>λ</em> = −1</span>处发生了什么，同样的<span
class="math inline"><em>c</em> = −1</span>，那么<span
class="math inline"><em>f</em>(−1) = 1 + (−1) · (−1) = 1 + 1 = 2</span>，这与我们上面所提出的一致！误差不仅没变小，反而变大了一倍。所以GMRES在构造最优多项式时，会把系数<strong>0</strong>分配给奇数步。</li>
</ul></li>
</ul>
<p>所以这就说明了为什么<span
class="math inline"><em>A</em><sub>2</sub></span>的收敛图像会时不时出现一条折线：折线是在奇数步产生的，在奇数步时算法给奇次项系数分配了0这个数，这就会导致在奇数步时实际的多项式仍然与上一步（偶数）的相同，导致误差完全没有发生变化。所以画出来的局部误差图是一条从平滑下降直线突变到斜率为0的直线。拓展到整个迭代过程，就产生了断断续续的折线。</p>
<p>那我们还是很想知道的是，为什么GMRES算法会能给奇次项分配<strong>0</strong>？
其实GMRES并不是主动去选0，而是因为在奇数步时，生成的那个新方向，刚好和我们想去的方向垂直。</p>
<p>GMRES 算法在第 <span class="math inline"><em>k</em></span>
步时试图寻找一 个系数 <span
class="math inline"><em>α</em></span>，使得新残差 <span
class="math inline">||<strong>r</strong><sub><em>n</em><em>e</em><em>w</em></sub>|| = ||<strong>r</strong><sub><em>o</em><em>l</em><em>d</em></sub> − <em>α</em><strong>q</strong><sub><em>n</em><em>e</em><em>w</em></sub>||</span>
最小。根据最小二乘法原理，最优的 <span
class="math inline"><em>α</em></span> 实际上是 <span
class="math inline"><strong>r</strong><sub><em>o</em><em>l</em><em>d</em></sub></span>
在新方向 <span
class="math inline"><strong>q</strong><sub><em>n</em><em>e</em><em>w</em></sub></span>
上的<strong>正交投影</strong>：</p>
<p><span class="math display">$$
\alpha = \frac{\mathbf{r}_{old} \cdot
\mathbf{q}_{new}}{||\mathbf{q}_{new}||^2}
$$</span></p>
<p>在奇数步时，由于矩阵特征值的对称性以及初始向量的平衡性，我们要证明在当前残差向量
<span
class="math inline"><strong>r</strong><sub><em>o</em><em>l</em><em>d</em></sub></span>
与新生成的方向 <span
class="math inline"><strong>q</strong><sub><em>n</em><em>e</em><em>w</em></sub></span>
是相互垂直（正交）的，即点积为 0。</p>
<p><strong>为什么奇数步时新方向与残差向量方向垂直？</strong>
因为我们有两个关于远点对称的特征值，所以我们可以认为，残差向量<span
class="math inline"><em>r</em></span>，一定由这两个特征值对应的特征向量组成，且这两个向量正交。
我们<strong>假设r在正负两个方向的分量长度相等（”平衡“）</strong>。我们可以把<span
class="math inline"><strong>r</strong></span>写成：<span
class="math inline"><em>r</em> = <em>u</em> + <em>v</em></span>，显然<span
class="math inline">||<em>u</em>|| = ||<em>v</em>||</span>。我们尝试迈出奇数步，算出新方向<span
class="math inline"><em>q</em><sub><em>n</em><em>e</em><em>w</em></sub> = <em>A</em><em>r</em></span>，因为<span
class="math inline"><em>r</em></span>的分解，所以我们又可以写成<span
class="math inline"><em>A</em><em>r</em> = <em>λ</em><em>u</em> − <em>λ</em><em>v</em> = <em>λ</em>(<em>u</em> − <em>v</em>)</span>。现在我们来看残差向量和新向量的点积</p>
<p><span class="math display">$$
           \begin{aligned}
           r·(Ar) &amp;= (u+v)·\lambda(u-v) \\
                  &amp;= \lambda[(u+v)(u-v)] \\
                  &amp;= \lambda[u·u-u·v+v·u-v·v] \\
                  &amp;= \lambda(||u||^2-||v||^2)
           \end{aligned}
           $$</span></p>
<p>因为<span class="math inline"><em>u</em></span>和<span
class="math inline"><em>v</em></span>的大小相等，所以 <span
class="math display"><em>r</em>(<em>A</em><em>r</em>) = <em>λ</em>(||<em>u</em>||<sup>2</sup> − ||<em>v</em>||<sup>2</sup>) = 0</span></p>
<p>点积为0，说明两者确实是垂直的。 而且因为我们设定的<span
class="math inline"><em>b</em></span>是全1向量，这就导致<span
class="math inline"><em>b</em></span>在正特征值空间的分量，和在负特征值空间的分量，<strong>恰好大小相等</strong>
正是这种完美的对称性，导致了完美的垂直，进而导致了完美的停滞（横线）。
如果<span
class="math inline"><em>b</em></span>是随机生成的向量，那么<span
class="math inline">||<em>u</em>||</span>和<span
class="math inline">||<em>v</em>||</span>就不一定完全相等，垂直条件就会被破坏，那条横线可能就会变成一条”稍微有点斜率“的线，但依然会很慢</p>
<p>那我们在推理完为什么图像会有”折线“出现时，我们对于<span
class="math inline"><strong>A</strong><sub><strong>2</strong></sub></span>的收敛速度的问题就十分显然了</p>
<p>因为奇次项的系数为0，我们就相当于在这个什么都没干，就相当于这个奇次项<strong>停摆</strong>了。所以每当迭代到奇数步时，我们的误差收敛会停滞。这就意味着，我们对比<span
class="math inline"><em>A</em><sub>1</sub></span>执行了<span
class="math inline"><em>m</em></span>次迭代，由于我们的奇数步”瘫痪了“，实际上<span
class="math inline"><em>A</em><sub>2</sub></span>只迭代了<span
class="math inline">$\frac{m}{2}$</span>次！换句话说，对于<span
class="math inline"><em>A</em><sub>2</sub></span>，我们其实是在构建<span
class="math inline"><em>p</em>(<em>A</em><sub>2</sub><sup>2</sup>)</span>，有效迭代次数直接砍半了，自然地，收敛速度也会收到影响。这就解释了图像上显示的<span
class="math inline"><em>A</em><sub>2</sub></span>收敛速度显著慢于<span
class="math inline"><em>A</em><sub>1</sub></span>的现象。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2026/01/23/%E6%95%B0%E5%80%BC%E5%88%86%E6%9E%90/260123_%E7%89%B9%E5%BE%81%E5%80%BC%E5%88%86%E5%B8%83%E7%9A%84%E5%AF%B9%E7%A7%B0%E6%80%A7%E5%AF%B9%E4%BA%8E%E8%AF%AF%E5%B7%AE%E7%9A%84%E5%BD%B1%E5%93%8D%E7%9A%84%E8%A7%A3%E9%87%8A/" data-id="cuidRo3PKwS8S9UxIO5PBmrtZ" data-title="特征值分布的对称性对于误差的影响的解释" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E6%95%B0%E5%80%BC%E5%88%86%E6%9E%90/" rel="tag">数值分析</a></li></ul>

    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2026/02/08/hello-world/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          Hello World
        
      </div>
    </a>
  
  
</nav>

  
</article>


</section>
        
          <aside id="sidebar">
  
    

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tags</h3>
    <div class="widget">
      <ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" rel="tag">卷积神经网络</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%95%B0%E5%80%BC%E5%88%86%E6%9E%90/" rel="tag">数值分析</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" rel="tag">深度学习</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%EF%BC%8C%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/" rel="tag">深度学习，线性回归</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/" rel="tag">线性回归</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" style="font-size: 10px;">卷积神经网络</a> <a href="/tags/%E6%95%B0%E5%80%BC%E5%88%86%E6%9E%90/" style="font-size: 15px;">数值分析</a> <a href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" style="font-size: 20px;">深度学习</a> <a href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%EF%BC%8C%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/" style="font-size: 15px;">深度学习，线性回归</a> <a href="/tags/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/" style="font-size: 15px;">线性回归</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2026/02/">February 2026</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2026/01/">January 2026</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2026/02/08/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Adam%E7%AE%97%E6%B3%95/">Adam算法</a>
          </li>
        
          <li>
            <a href="/2026/02/08/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/%E6%95%B4%E7%90%86%E4%B8%80%E4%B8%8Bsoftmax%E5%9B%9E%E5%BD%92%E5%AE%9E%E7%8E%B0%E4%B8%AD%E8%AE%AD%E7%BB%83%E9%83%A8%E5%88%86%E4%BB%A3%E7%A0%81%E7%9A%84%E6%80%9D%E8%B7%AF/">整理一下softmax回归实现中训练部分代码的思路</a>
          </li>
        
          <li>
            <a href="/2026/02/08/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/LossSumExp%E6%8A%80%E5%B7%A7%EF%BC%88LSE%EF%BC%89/">LossSumExp技巧（LSE）</a>
          </li>
        
          <li>
            <a href="/2026/02/08/%E6%95%B0%E5%80%BC%E5%88%86%E6%9E%90/%E5%85%B1%E8%BD%AD%E6%A2%AF%E5%BA%A6%E6%B3%95%E4%B8%AD%E5%8F%82%E6%95%B0alpha%E5%92%8Cbeta%E7%9A%84%E6%8E%A8%E5%AF%BC/">共轭梯度法中参数alpha和beta的推导</a>
          </li>
        
          <li>
            <a href="/2026/02/08/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/%E5%8A%A8%E9%87%8F%E6%B3%95/">动量法</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      
      &copy; 2026 John Doe<br>
      Powered by <a href="https://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>

    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    


<script src="/js/jquery-3.6.4.min.js"></script>



  
<script src="/fancybox/jquery.fancybox.min.js"></script>




<script src="/js/script.js"></script>






<!-- 强制注入 MathJax -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
  </div>
<!-- hexo injector body_end start -->
  <script src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"></script>
  <script>
    MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [ ['$','$'], ["\(","\)"] ],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
    });
  </script>
<!-- hexo injector body_end end --></body>
</html>