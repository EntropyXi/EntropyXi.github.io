[{"title":"Hello World","url":"/2026/02/08/hello-world/","content":"Welcome to Hexo! This is your very\r\nfirst post. Check documentation for\r\nmore info. If you get any problems when using Hexo, you can find the\r\nanswer in troubleshooting or\r\nyou can ask me on GitHub.\r\nQuick Start\r\nCreate a new post\r\n$ hexo new &quot;My New Post&quot;\r\nMore info: Writing\r\nRun server\r\n$ hexo server\r\nMore info: Server\r\nGenerate static files\r\n$ hexo generate\r\nMore info: Generating\r\nDeploy to remote sites\r\n$ hexo deploy\r\nMore info: Deployment\r\n"},{"title":"特征值分布的对称性对于误差的影响的解释","url":"/2026/01/23/%E6%95%B0%E5%80%BC%E5%88%86%E6%9E%90/260123_%E7%89%B9%E5%BE%81%E5%80%BC%E5%88%86%E5%B8%83%E7%9A%84%E5%AF%B9%E7%A7%B0%E6%80%A7%E5%AF%B9%E4%BA%8E%E8%AF%AF%E5%B7%AE%E7%9A%84%E5%BD%B1%E5%93%8D%E7%9A%84%E8%A7%A3%E9%87%8A/","content":"我们设I是一个100 × 100单位矩阵，Z是一个100 × 100的零矩阵，b是一个200 × 1的全1向量 设$\\mathbf{B}=\\begin{bmatrix} 1 &amp; &amp; &amp; \\\\\r\n&amp; 2 &amp; &amp; \\\\ &amp; &amp; \\ddots &amp; \\\\ &amp; &amp; &amp; 100\r\n\\end{bmatrix}$ 然后，令$\\mathbf{A_1}=\\begin{bmatrix} \\mathbf{B} &amp;\r\n\\mathbf{I} \\\\ \\mathbf{Z} &amp; \\mathbf{B} \\end{bmatrix}$ ，$\\mathbf{A_2}=\\begin{bmatrix} \\mathbf{B} &amp;\r\n\\mathbf{I} \\\\ \\mathbf{Z} &amp; \\mathbf{-B} \\end{bmatrix}$\r\n我们使用GMRES算法，分别绘制A1与A2的残差收敛图，如下图所示\r\n\r\n我们关注两个问题\r\n1.为什么A1的收敛曲线是光滑的，而A2的是带有折线的？\r\n2.为什么A1收敛得更快，而A2收敛极其缓慢？\r\n我们先来回顾一下Arnoldi迭代 Arnoldi\r\n迭代本质上是在构造一组正交多项式 我们生成的基向量q1, q2, q3…，其实他们代表了A的不同次幂作用在b上 - q1：它是b的归一化\r\n本质：零次多项式ϕ0(A)b\r\n- q2：来自Aq1（并减去了q1分量） 本质：包含Ab和b，即1次多项式ϕ1(A)b\r\n- q3：来自Aq2\r\n本质：2次多项式ϕ2(A)b\r\n这意味着，你的矩阵Qm的每一列，本质上都是一个以向量形式存在的，作用在b上的多项式\r\n然后，GMRES算法本质上是在求解一个最小二乘问题，也就是我们通过了这个算法得到了一个用来”形容“z的最优多项式，\r\nmin ||Hmz − ||b||e1||\r\n我们会得到一个解向量y = [y1, y2, …, ym]T（就是z）\r\n这是配方，它告诉我们对于多项式的每一项，我们应该取多少项的qm 因为y是让残差最小的“最优配方”，所以它组合出来的多项式，自然就是最优多项式\r\n而我们去寻找这个最优多项式，要去令残差r = p(A)b最小，本质上也是去把多项式中特征值的位置（大小）压低。\r\n这是什么意思？ 我们知道，在矩阵A张成的空间里，任何一个向量b都可以写成这些特征向量的叠加：\r\nb = y1v1 + y2v2 + … + ynvn\r\n又有p(A)vi = p(λi)vi，所以，残差r = p(A)b就可以表示为\r\n$$\r\n\\begin{aligned}\r\n\\mathbf{r} &amp;= p(A)\\mathbf{b} \\\\\r\n           &amp;= p(A)(y_1\\mathbf{y_1}+\\dots+y_n\\mathbf{v_n}) \\\\\r\n           &amp;=\r\ny_1p(\\lambda_1)\\mathbf{v_1}+\\dots+y_np(\\lambda_n)\\mathbf{v_n}\r\n\\end{aligned}\r\n$$\r\n所以\r\n||r|| ≈ ∑||cip(λi)vi||\r\n我们会发现，如果我们要让残差最小，我们得让p(λ)最小，如果你想让残差逼近0，那你就得让p(λ) = 0\r\n现在准备工作准备得差不多，我们可以开始看问题了\r\n\r\n首先，为什么A2的收敛图像会带有折线？\r\n观察A2的结构，它是一个稀疏矩阵且对角线上的元素值是关于原点对称的，所以我们可以近似认为，这个矩阵的特征值是关于原点对称的。\r\n我们注意到，奇数步时多项式的最高次项是奇数项\r\n\r\n而奇次函数是反对称的：f(λ) = −f(−λ)\r\n这就意味这，如果你想去用奇次项去压低正特征值处的误差，负特征值处的误差就会被放大了。例如，现在我们有一个奇次函数f(x) = 1 + cx，矩阵有特征值1和-1，我们先令f(1) ≈ 0，即1 + c · (1) ≈ 0，我们取理想值c = −1，此时p(1) = 1 + (−1) · 1 = 0，正特征值处的误差被消除了。那我们再看λ = −1处发生了什么，同样的c = −1，那么f(−1) = 1 + (−1) · (−1) = 1 + 1 = 2，这与我们上面所提出的一致！误差不仅没变小，反而变大了一倍。所以GMRES在构造最优多项式时，会把系数0分配给奇数步。\r\n\r\n\r\n所以这就说明了为什么A2的收敛图像会时不时出现一条折线：折线是在奇数步产生的，在奇数步时算法给奇次项系数分配了0这个数，这就会导致在奇数步时实际的多项式仍然与上一步（偶数）的相同，导致误差完全没有发生变化。所以画出来的局部误差图是一条从平滑下降直线突变到斜率为0的直线。拓展到整个迭代过程，就产生了断断续续的折线。\r\n那我们还是很想知道的是，为什么GMRES算法会能给奇次项分配0？\r\n其实GMRES并不是主动去选0，而是因为在奇数步时，生成的那个新方向，刚好和我们想去的方向垂直。\r\nGMRES 算法在第 k\r\n步时试图寻找一 个系数 α，使得新残差 ||rnew|| = ||rold − αqnew||\r\n最小。根据最小二乘法原理，最优的 α 实际上是 rold\r\n在新方向 qnew\r\n上的正交投影：\r\n$$\r\n\\alpha = \\frac{\\mathbf{r}_{old} \\cdot\r\n\\mathbf{q}_{new}}{||\\mathbf{q}_{new}||^2}\r\n$$\r\n在奇数步时，由于矩阵特征值的对称性以及初始向量的平衡性，我们要证明在当前残差向量\r\nrold\r\n与新生成的方向 qnew\r\n是相互垂直（正交）的，即点积为 0。\r\n为什么奇数步时新方向与残差向量方向垂直？\r\n因为我们有两个关于远点对称的特征值，所以我们可以认为，残差向量r，一定由这两个特征值对应的特征向量组成，且这两个向量正交。\r\n我们假设r在正负两个方向的分量长度相等（”平衡“）。我们可以把r写成：r = u + v，显然||u|| = ||v||。我们尝试迈出奇数步，算出新方向qnew = Ar，因为r的分解，所以我们又可以写成Ar = λu − λv = λ(u − v)。现在我们来看残差向量和新向量的点积\r\n$$\r\n           \\begin{aligned}\r\n           r·(Ar) &amp;= (u+v)·\\lambda(u-v) \\\\\r\n                  &amp;= \\lambda[(u+v)(u-v)] \\\\\r\n                  &amp;= \\lambda[u·u-u·v+v·u-v·v] \\\\\r\n                  &amp;= \\lambda(||u||^2-||v||^2)\r\n           \\end{aligned}\r\n           $$\r\n因为u和v的大小相等，所以 r(Ar) = λ(||u||2 − ||v||2) = 0\r\n点积为0，说明两者确实是垂直的。 而且因为我们设定的b是全1向量，这就导致b在正特征值空间的分量，和在负特征值空间的分量，恰好大小相等\r\n正是这种完美的对称性，导致了完美的垂直，进而导致了完美的停滞（横线）。\r\n如果b是随机生成的向量，那么||u||和||v||就不一定完全相等，垂直条件就会被破坏，那条横线可能就会变成一条”稍微有点斜率“的线，但依然会很慢\r\n那我们在推理完为什么图像会有”折线“出现时，我们对于A2的收敛速度的问题就十分显然了\r\n因为奇次项的系数为0，我们就相当于在这个什么都没干，就相当于这个奇次项停摆了。所以每当迭代到奇数步时，我们的误差收敛会停滞。这就意味着，我们对比A1执行了m次迭代，由于我们的奇数步”瘫痪了“，实际上A2只迭代了$\\frac{m}{2}$次！换句话说，对于A2，我们其实是在构建p(A22)，有效迭代次数直接砍半了，自然地，收敛速度也会收到影响。这就解释了图像上显示的A2收敛速度显著慢于A1的现象。\r\n","tags":["数值分析"]},{"title":"Adam算法","url":"/2026/02/08/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Adam%E7%AE%97%E6%B3%95/","content":"\r\n/* 强制让 MathJax 公式容器支持横向滚动 */\r\n.mjx-container, .MathJax_Display, .MathJax {\r\n    overflow-x: auto !important; /* 超出宽度时显示滚动条 */\r\n    overflow-y: hidden;          /* 隐藏垂直滚动条 */\r\n    max-width: 100%;             /* 限制最大宽度为屏幕宽度 */\r\n    -webkit-overflow-scrolling: touch; /* 优化移动端滑动体验 */\r\n}\r\n\r\n偏差\r\n我们完成了Momentum和RMSProp 如果我们将两者拼凑在一起 1.\r\n更新一阶矩（动量）\r\nmt = β1mt − 1 + (1 − β1)gt\r\n\r\n更新二阶矩（自适应项）\r\n\r\nvt = β2vt − 1 + (1 − β2)gt2\r\n\r\n参数更新\r\n\r\n$$\r\n\\mathbf{x}_{t+1} = \\mathbf{x}_t - \\eta \\frac{m_t}{\\sqrt{v_t} + \\epsilon}\r\n$$\r\n其中 β1 通常为\r\n0.9，β2 通常为\r\n0.999 接下来让我们看看当t = 1时发生了什么 假设我们将m0, v0初始化为0向量 带入公式\r\nv1 = β2 ⋅ 0 + (1 − β2)g12 = (1 − 0.999)g12 = 0.001 ⋅ g12\r\n我们观察到，计算出的二阶矩估计值仅为真实梯度平方的0.001倍！\r\n这意味着估计值严重偏向于0 如果直接用这个vt去做分母$\\sqrt{v_1}$，分母会极小，导致更新步长会爆炸性地变大（或者在\r\nmt\r\n上导致步长极小，取决于分子分母谁偏得更多）\r\n这种初始化偏差会导致训练初期极其不稳定\r\n修正\r\n为了消除这个偏差，我们需要从统计学角度推导一个修正系数\r\n我们先作一个期望分析\r\n假设真实梯度的二阶矩是平稳的（Stationary），记为 E[g2]。我们希望我们的估计量\r\nvt\r\n是无偏的，即希望 E[vt] = E[g2]\r\n让我们展开 vt 的递归式\r\n$$\r\nv_t = (1-\\beta_2) \\sum_{i=1}^t \\beta_2^{t-i} \\mathbf{g}_i^2\r\n$$\r\n对两边求期望 E[⋅]\r\n$$\r\n\\begin{aligned}\r\nE[v_t] &amp;= E\\left[ (1-\\beta_2) \\sum_{i=1}^t \\beta_2^{t-i}\r\n\\mathbf{g}_i^2 \\right] \\\\ &amp;= (1-\\beta_2) \\sum_{i=1}^t \\beta_2^{t-i}\r\nE[\\mathbf{g}_i^2] \\\\ &amp;\\approx (1-\\beta_2) E[\\mathbf{g}^2]\r\n\\sum_{i=1}^t \\beta_2^{t-i}\r\n\\end{aligned}\r\n$$\r\n这里 $\\sum_{i=1}^t \\beta_2^{t-i}$\r\n是一个等比数列求和，其值为\r\n$$\r\n\\sum_{k=0}^{t-1} \\beta_2^k = \\frac{1 - \\beta_2^t}{1 - \\beta_2}\r\n$$\r\n代回期望公式\r\n$$\r\n\\begin{aligned}\r\nE[v_t] &amp;\\approx (1-\\beta_2) E[\\mathbf{g}^2] \\cdot \\frac{1 -\r\n\\beta_2^t}{1 - \\beta_2} \\\\ &amp;= E[\\mathbf{g}^2] \\cdot (1 - \\beta_2^t)\r\n\\end{aligned}$$\r\n我们发现，\r\nE[vt] = 真实值 × (1 − β2t)\r\n为了得到真实值，我们必须人为的除以系数1 − β2t\r\n- 当 t 很小时，β2t = 0.999，修正因子\r\n1 − 0.999 = 0.001。我们把 vt 除以 0.001，正好把它放大了 1000 倍，还原了真实量级\r\n- 当 t 很大时，β2t → 0，修正因子\r\n → 1。此时不再需要修正，因为 EMA\r\n已经积累了足够的数据，偏差自然消失了 同理，对一阶矩 mt 也需要除以\r\n(1 − β1t)\r\n结合上述所有推导，我们得到完整的Adam算法流程 迭代过程 (在时刻\r\nt)：\r\n计算梯度：\r\ngt = ∇f(xt − 1)\r\n更新一阶矩：\r\nmt = β1mt − 1 + (1 − β1)gt\r\n更新二阶矩：\r\nvt = β2vt − 1 + (1 − β2)gt2\r\n计算偏差修正后的一阶矩：\r\n$$\r\n\\hat{m}_t = \\frac{m_t}{1 - \\beta_1^t}\r\n$$\r\n计算偏差修正后的二阶矩：\r\n$$\r\n\\hat{v}_t = \\frac{v_t}{1 - \\beta_2^t}\r\n$$\r\n计算参数更新：\r\n$$\r\n\\mathbf{x}_t = \\mathbf{x}_{t-1} - \\alpha \\cdot\r\n\\frac{\\hat{m}_t}{\\sqrt{\\hat{v}_t} + \\epsilon}\r\n$$\r\n","tags":["深度学习"]},{"title":"CNN的设计原则与数学推导","url":"/2026/02/08/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/CNN/CNN%E7%9A%84%E8%AE%BE%E8%AE%A1%E5%8E%9F%E5%88%99%E4%B8%8E%E6%95%B0%E5%AD%A6%E6%8E%A8%E5%AF%BC/","content":"\r\n/* 强制让 MathJax 公式容器支持横向滚动 */\r\n.mjx-container, .MathJax_Display, .MathJax {\r\n    overflow-x: auto !important; /* 超出宽度时显示滚动条 */\r\n    overflow-y: hidden;          /* 隐藏垂直滚动条 */\r\n    max-width: 100%;             /* 限制最大宽度为屏幕宽度 */\r\n    -webkit-overflow-scrolling: touch; /* 优化移动端滑动体验 */\r\n}\r\n\r\n原则一：平移不变性\r\n假设我们需要检测图像中是否有“一只猫”。无论这只猫位于图像的左上角\r\n(x1, y1)\r\n还是右下角 (x2, y2)，它都是猫。\r\n在全连接网络中，识别左上角的权重 wtop_left\r\n和识别右下角的权重 wbottom_right\r\n是完全独立的。这意味着模型必须在不同位置重新学习什么是“猫\r\n我们采用参数共享的解决方案\r\n我们不应该为图像的每个位置学习单独的检测器，而应该定义一个通用的检测器（卷积核、滤波器），让它在整个图像上滑动\r\n- 如果在位置A检测到了特征f，那么同样的检测器在位置B也应该能检测到特征f - 数学上，这意味着函数f 对于输入 x 的平移操作 T 满足 f(T(x)) = f(x)\r\n原则二：局部性定理\r\n全连接试图试图建立所有像素间的联系，这是及其耗费内存的。因为图像像素具有很强的空间相关性，即像素\r\nxi, j\r\n的值与其邻域（如 xi + 1, j）高度相关，但与距离很远的像素（如\r\nxi + 500, j + 500）的相关性几乎为零\r\n所以我们采取稀疏连接\r\n我们限制神经元只与输入图像的一个小窗口（例如3 × 3或者5 × 5）相连，这个区域充分地减少了计算量\r\n原则三：层级与感受野\r\n虽然第一层只关注局部，但我们最终需要识别整张图像。局部的线条如何组合成形状，形状又如何组合成物体？\r\n我们使用感受野扩张 - 第 1 层神经元看到 3 × 3 的像素 - 第 2 层神经元看到第 1 层的\r\n3 × 3\r\n个输出，这实际上覆盖了原始图像中更大的区域 -\r\n随着层数加深，高层神经元能“看见”原始图像中更广阔的区域，从而捕获全局语义信息\r\n数学形式化\r\n我们如何从全连接公式变为卷积公式？ 对于输入图像 X (展开为一维向量) 和输出 H，全连接层定义为\r\nhi = ∑jWi, jxj\r\n其中 Wi, j\r\n表示第 j 个输入像素对第 i\r\n个输出神经元的权重。这包含了所有可能的相互作用\r\n根据原则二，神经元 i 只应当受其附近的输入 j 影响。如果 |i − j| &gt; Δ（Δ 为邻域范围），则强制 Wi, j = 0。公式变为\r\n$$\r\nh_i = \\sum_{j=i-\\Delta}^{i+\\Delta} W_{i,j} x_j\r\n$$\r\n此时矩阵 W\r\n变成了带状矩阵，这是一个稀疏矩阵\r\n根据原则一，检测特征的权重不应依赖于具体位置 i，即 Wi, j\r\n不应该取决于 i，而只取决于输入和输出的相对位置\r\n(i − j) 我们将 Wi, j\r\n替换为 wi − j（这里\r\nw 就是我们的卷积核）\r\n$$\r\nh_i = \\sum_{j=i-\\Delta}^{i+\\Delta} w_{i-j} x_j\r\n$$\r\n如果我们令 a = i − j，则\r\nj = i − a。通过变量代换，我们得到标准的离散卷积\r\n$$\r\nh_i = \\sum_{a=-\\Delta}^{\\Delta} w_a x_{i-a}\r\n$$\r\n","tags":["卷积神经网络"]},{"title":"RMSProp","url":"/2026/02/08/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/RMSProp/","content":"\r\n/* 强制让 MathJax 公式容器支持横向滚动 */\r\n.mjx-container, .MathJax_Display, .MathJax {\r\n    overflow-x: auto !important; /* 超出宽度时显示滚动条 */\r\n    overflow-y: hidden;          /* 隐藏垂直滚动条 */\r\n    max-width: 100%;             /* 限制最大宽度为屏幕宽度 */\r\n    -webkit-overflow-scrolling: touch; /* 优化移动端滑动体验 */\r\n}\r\n\r\nRMSProp 是一种\r\n自适应学习率方法。它通过计算梯度的二阶矩的指数加权移动平均，来对每个参数的学习率进行独立缩放\r\n$$\r\n\\begin{aligned}\r\nE[\\mathbf{g}^2]_t &amp;= \\beta E[\\mathbf{g}^2]_{t-1} + (1-\\beta)\r\n(\\mathbf{g}_t \\odot \\mathbf{g}_t) \\\\\r\n\\mathbf{x}_{t+1} &amp;= \\mathbf{x}_t -\r\n\\frac{\\eta}{\\sqrt{E[\\mathbf{g}^2]_t + \\epsilon}} \\odot \\mathbf{g}_t\r\n\\end{aligned}\r\n$$\r\n其中\r\n\r\n⊙ 表示哈达玛积，即逐元素相乘。\r\nE[g2]t\r\n是梯度平方的累积量。\r\nβ\r\n是衰减率（典型值为 0.9 或 0.99）。\r\nϵ\r\n是防止分母为零的平滑项。\r\n\r\n构造与推导\r\n我们从RMSProp的根源，AdaGrad开始讲起\r\n动量法解决了全局震荡的问题，但是我们的所有参数共用一个学习率η\r\n在处理稀疏特征或尺度差异巨大的参数（例如深度nn的网络）时，我们希望\r\n\r\n对于频繁更新或梯度很大的参数，降低其学习率，防止其发散\r\n对于频繁更新或梯度很大的参数，增大其学习率\r\n\r\n为了实现上述目标，我们需要构造一个调节器，这个调节器应该与该方向上的历史梯度的幅度成反比\r\n那么，我们该如何衡量一个参数在过去一段时间内的梯度大小？\r\n简单的累加梯度∑gt肯定是不行的，因为正负梯度会互相抵消（实际上是剧烈震荡）\r\n因此，我们必须累加梯度的平方 定义rt为知道t时刻的所有梯度的平方和\r\n$$\r\nr_t=\\sum_{\\tau=1}^{t}g_\\tau^2\r\n$$\r\n现在，我们使用rt作为分母，对全局学习率进行归一化，这就是AdaGrad的核心公式\r\n$$\r\nw_{i+1}=w_i-\\frac{\\eta}{r_t+\\epsilon}\\odot g_t\r\n$$\r\n\r\nϵ是一个防止分母为0的常数\r\n\r\nAdaGrad的致命缺陷\r\n虽然AdaGrad解决了“各向异性”问题，但是在深层神经网络学习的优化场景中，他暴露出了一个致命的问题\r\n我们观察rt的定义\r\n$$\r\nr_t = \\sum_{\\tau=1}^{t} g_\\tau^2 = r_{t-1} + g_t^2\r\n$$\r\n由于gt2是非负的，我们会发现，整个式子是单调递增的，也就是说rt是单调递增的！\r\n随着训练迭代次数t趋近于∞\r\n\r\nrt → ∞\r\n有效学习率$\\frac{\\eta}{\\sqrt{r_t+\\epsilon}}\\to0$\r\n\r\n如果在找到最小值之前，学习率就已经减小到几乎为0，参数更新就会停止，无法到达最优解，这在深度神经网络中是不可接受的\r\n所以我们引入RMSProp修正\r\n我们需要一种方法，既能衡量梯度的大小，又只关注最近的历史，而不是从盘古开天辟地（t = 1）开始累积\r\n我们可以通过指数加权移动平均来实现。我们不再直接求和，而是使用一个衰减系数β\r\nRMSProp 的构造步骤\r\n我们将 AdaGrad 的累积公式：\r\nrt = rt − 1 + gt2\r\n修改为 RMSProp 的加权更新公式：\r\n$$\r\nE[g^2]_t = \\beta \\cdot \\underbrace{E[g^2]_{t-1}}_{\\text{历史信息}} +\r\n(1-\\beta) \\cdot \\underbrace{g_t^2}_{\\text{当前信息}}\r\n$$\r\n步骤解析： 1. 每经过一步，旧的累积量 E[g2]t − 1\r\n就会乘以 β（例如\r\n0.9）。这意味着很久以前的梯度信息会以 0.9, 0.81, 0.729...\r\n的速度迅速衰减，不再主导分母 2. 当前的梯度平方 gt2\r\n被赋予权重 (1 − β) 加入累积量\r\n3. 这种方式使得 E[g2]t\r\n不再是单调递增的，它变成了一个在该参数梯度平均强度附近的滑动窗口估计量\r\n结合上述推导，我们得到了RMSProp的完整形式 对于参数 w 和梯度 gt -\r\n计算梯度平方的移动平均\r\nvt = βvt − 1 + (1 − β)gt2\r\n这里常用符号 vt 或 ht 表示累积量\r\nE[g2]t\r\n\r\n参数更新\r\n\r\n$$\r\nw_{t+1} = w_t - \\frac{\\eta}{\\sqrt{v_t + \\epsilon}} g_t\r\n$$\r\n这就得到了我们最后的公式\r\n","tags":["深度学习"]},{"title":"test-post","url":"/2026/02/08/test-post/","content":"\r\n"},{"title":"动量法","url":"/2026/02/08/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/%E5%8A%A8%E9%87%8F%E6%B3%95/","content":"\r\n/* 强制让 MathJax 公式容器支持横向滚动 */\r\n.mjx-container, .MathJax_Display, .MathJax {\r\n    overflow-x: auto !important; /* 超出宽度时显示滚动条 */\r\n    overflow-y: hidden;          /* 隐藏垂直滚动条 */\r\n    max-width: 100%;             /* 限制最大宽度为屏幕宽度 */\r\n    -webkit-overflow-scrolling: touch; /* 优化移动端滑动体验 */\r\n}\r\n\r\n动量法旨在加速 SGD\r\n在相关方向上的收敛，并抑制无关方向上的震荡。在时间步 t，给定目标函数 f(x) 关于参数 x 的梯度 gt = ∇xf(xt)，引入速度向量\r\nvt，其更新公式如下\r\n$$\r\n\\begin{aligned}\r\n\\mathbf{v}_{t+1} &amp;= \\mu \\mathbf{v}_t + \\eta \\mathbf{g}_t \\\\\r\n\\mathbf{x}_{t+1} &amp;= \\mathbf{x}_t - \\mathbf{v}_{t+1}\r\n\\end{aligned}\r\n$$\r\n其中，μ ∈ [0, 1) 为\r\n动量系数（通常取 0.9），η 为学习率\r\n构造与推导\r\n考虑一个局部呈现为狭长山谷状地损失函数表面。如果不适用动量，SGD的更新公式为xt + 1 = xt − ηgt，此时出现两个问题\r\n\r\n在山谷的横截面方向（高曲率，梯度大），gt很大，导致参数在山谷两壁间剧烈震荡\r\n在山谷的延伸方向（低曲率，梯度小），gt很小，导致收敛的速度极慢\r\n\r\n而我们所需要的是，我们希望算法能够记住之前的更新方向，在震荡方向上相互抵消，在平缓方向上累积速度\r\n我们需要引入一个时间平滑机制\r\n\r\n如果连续多次的方向一致（平滑），步长应增大\r\n如果连续梯度的方向相反（震荡），步长应减小\r\n\r\n所以，我们将当前的更新向量不再仅仅设为当前梯度，而是设为当前梯度与过去梯度的指数加权移动平均\r\n定义累积变量 vt。在\r\nt = 0 时，v0 = 0。 在 t = 1 时，v1 = ηg0。\r\n在 t = 2 时，我们要保留 v1\r\n的一部分，并加上新的梯度：v2 = μv1 + ηg1。\r\n展开递归式：\r\n$$\\mathbf{v}_{t+1} = \\eta \\mathbf{g}_t +\r\n\\mu (\\eta \\mathbf{g}_{t-1} + \\mu \\mathbf{v}_{t-1}) = \\eta\r\n\\sum_{\\tau=0}^{t} \\mu^{t-\\tau} \\mathbf{g}_\\tau$$\r\n观察此式：\r\n\r\n当梯度方向一致时（sign(gt) = sign(gt − 1)），v\r\n的模长会增大，起到加速作用。\r\n当梯度方向改变时（sign(gt) ≠ sign(gt − 1)），正负项在求和中互相抵消，起到抑制震荡作用。\r\n\r\n","tags":["深度学习","线性回归"]},{"title":"LossSumExp技巧（LSE）","url":"/2026/02/08/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/LossSumExp%E6%8A%80%E5%B7%A7%EF%BC%88LSE%EF%BC%89/","content":"\r\n/* 强制让 MathJax 公式容器支持横向滚动 */\r\n.mjx-container, .MathJax_Display, .MathJax {\r\n    overflow-x: auto !important; /* 超出宽度时显示滚动条 */\r\n    overflow-y: hidden;          /* 隐藏垂直滚动条 */\r\n    max-width: 100%;             /* 限制最大宽度为屏幕宽度 */\r\n    -webkit-overflow-scrolling: touch; /* 优化移动端滑动体验 */\r\n}\r\n\r\n在手动实现的softmax和交叉熵损失中，我们在这两步是分步计算的，即我们在计算评估函数Xw+b后，我们先调用softmax计算$\\frac{exp(x_i)}{\\sum\r\nexp(x_j)}$，然后再把计算出来的softmax概率直接传给 log\r\n计算交叉熵。但是这种分步计算数值是及其不稳定的，因为我们的softmax计算出来的数值再0到1的区间内，而当数值趋近于0时，很容易遇到数值下溢出导致log(0)报错。这在数值上是十分脆弱的。\r\n所以我们使用LSE技巧，在损失函数内部，将这两部合并计算 -\r\n核心逻辑：框架（如 PyTorch 的 nn.CrossEntropyLoss\r\n）要求你传入未规范化的预测 -\r\n内部黑盒：在损失函数内部，它并不会先算概率再算对数，而是利用数学恒等式将这两步合并计算\r\n- 数学原理：它计算的是log (∑exp(xi))，并使用了平移技巧（减去最大值c）\r\nLogSumExp(x) = c + log ∑exp(xi − c),  其中c=max(x)\r\n\r\n优势：这种合并计算的方式在计算机底层是极其稳定的，永远不会出现NaN（不是数字）或无穷大的错误\r\n\r\n接下来我们证明等价性： 其中softmax公式：$\\tilde{y}_i=\\frac{exp(x_i)}{\\sum_j\r\nexp(x_j)}$ 交叉熵损失（针对单样本）：L = −log (ỹy)\r\n将softmax的定义带入到损失函数L中\r\n$$\r\nL=-\\log(\\tilde{y}_i=\\frac{exp(x_i)}{\\sum_j exp(x_j)})\r\n$$\r\n利用对数的运算法则展开\r\nL = −[log (exp(xi)) − log(∑jexp(xj))]\r\n因为log (exp(x)) = x，所以\r\nL = −[xi − log(∑jexp(xj))]\r\n展开\r\nL = −xi + log (∑jexp(xj))\r\n得证\r\n","tags":["深度学习，线性回归"]},{"title":"随机梯度下降","url":"/2026/02/08/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/%E9%9A%8F%E6%9C%BA%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D/","content":"\r\n/* 强制让 MathJax 公式容器支持横向滚动 */\r\n.mjx-container, .MathJax_Display, .MathJax {\r\n    overflow-x: auto !important; /* 超出宽度时显示滚动条 */\r\n    overflow-y: hidden;          /* 隐藏垂直滚动条 */\r\n    max-width: 100%;             /* 限制最大宽度为屏幕宽度 */\r\n    -webkit-overflow-scrolling: touch; /* 优化移动端滑动体验 */\r\n}\r\n\r\n因为梯度下降法需要遍历整个数据集，而在数据集数量庞大的时候，这样做必然会导致爆炸式增长的算法时间复杂度。所以我们在采用梯度下降法进行优化时，我们要在原数据集中随机选取几个且不重复的数据作为子数据集来进行优化迭代。这一种梯度下降法我们就称为随机梯度下降\r\n","tags":["深度学习","线性回归"]},{"title":"整理一下softmax回归实现中训练部分代码的思路","url":"/2026/02/08/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/%E6%95%B4%E7%90%86%E4%B8%80%E4%B8%8Bsoftmax%E5%9B%9E%E5%BD%92%E5%AE%9E%E7%8E%B0%E4%B8%AD%E8%AE%AD%E7%BB%83%E9%83%A8%E5%88%86%E4%BB%A3%E7%A0%81%E7%9A%84%E6%80%9D%E8%B7%AF/","content":"流程总结\r\n在训练前，我们需要利用一些函数来评估模型的分类精度\r\n首先，如果y_hat是矩阵，那么假定第二个维度存储每个类的预测分数。\r\n我们使用argmax获得每行中最大元素的索引来获得预测类别。\r\n然后我们将预测类别与真实y元素进行比较。\r\n由于等式运算符“==”对数据类型很敏感，\r\n因此我们将y_hat的数据类型转换为与y的数据类型一致。\r\n结果是一个包含0（错）和1（对）的张量。\r\n最后，我们求和会得到正确预测的数量。 def accuracy(y_hat, y):     &quot;&quot;&quot;计算预测正确的数量&quot;&quot;&quot;    if len(y_hat.shape) &gt; 1 and y_hat.shape[1] &gt; 1: # 如果是一个矩阵        y_hat = y_hat.argmax(axis=1) # 获取每一行的最大值    cmp = y_hat.astype(y.dtype) == y # 判断最大值是否与y中的每一行真实的最大值相同    return float(cmp.astype(y.dtype).sum()) # 返回一个正确预测的数量\r\n同样，对于任意数据迭代器data_iter可访问的数据集，我们可以评估在任意模型net的精度\r\ndef evaluate_accuracy(net, data_iter):     &quot;&quot;&quot;计算在指定数据集上模型的精度&quot;&quot;&quot;    metric = Accumulator(2)  # 正确预测数、预测总数    for X, y in data_iter: # 在迭代器中获取数据        metric.add(accuracy(net(X), y), d2l.size(y)) # 计算正确数量    return metric[0] / metric[1] # 返回精度 这里我们定义了一个类Accmulator，用于对多个变量进行累加\r\nclass Accumulator:     &quot;&quot;&quot;在n个变量上累加&quot;&quot;&quot;    def __init__(self, n):        self.data = [0.0] * n # 在声明实例时n有多少就代表着他想返回的精度的条例有多少    def add(self, *args):\t    # 用于对列表中的每一项加上（正确数量，数据总量）        self.data = [a + float(b) for a, b in zip(self.data, args)]    def reset(self):        self.data = [0.0] * len(self.data)    def __getitem__(self, idx):\t    # 定义了__getitem__方法说明了这个类创建的实例可以被索引化        return self.data[idx]\r\n训练\r\n我们先考虑在一个轮次中我们是怎么训练的，这一切还是在代码里用注释讲清楚比较好\r\ndef train_epoch_ch3(net, train_iter, loss, updater):  #@save    &quot;&quot;&quot;训练模型一个迭代周期&quot;&quot;&quot;    # 将模型设置为训练模式 设置为训练模式时 模型会自动计算梯度    if isinstance(net, torch.nn.Module):        net.train()    # 此时metric.add()方法能返回的是训练损失总和、训练准确度总和、样本数    metric = Accumulator(3)    for X, y in train_iter:        # 计算梯度并更新参数        y_hat = net(X)        # 计算损失        l = loss(y_hat, y)        # 在这里开始我们根据优化器的情况分成两种情况        if isinstance(updater, torch.optim.Optimizer):            # 使用PyTorch内置的优化器和损失函数            updater.zero_grad() # 梯度清空 不然梯度就会被累加            l.mean().backward() # 计算平均梯度            updater.step() # 自动管理 net.parameters()        else:            # 使用定制的优化器和损失函数            l.sum().backward() # 计算总梯度            updater(X.shape[0]) # 把总梯度平均化        metric.add(float(l.sum()), accuracy(y_hat, y), y.numel())    # 返回训练损失和训练精度    return metric[0] / metric[2], metric[1] / metric[2] 训练的完整实现 def train_ch3(net, train_iter, test_iter, loss, num_epochs, updater):  #@save    &quot;&quot;&quot;训练模型&quot;&quot;&quot;    &#x27;&#x27;&#x27;    animator = Animator(xlabel=&#x27;epoch&#x27;, xlim=[1, num_epochs], ylim=[0.3, 0.9],                        legend=[&#x27;train loss&#x27;, &#x27;train acc&#x27;, &#x27;test acc&#x27;])    &#x27;&#x27;&#x27;    for epoch in range(num_epochs):        train_metrics = train_epoch_ch3(net, train_iter, loss, updater) # 训练一次        test_acc = evaluate_accuracy(net, test_iter) # 测试集的精度        &#x27;&#x27;&#x27;animator.add(epoch + 1, train_metrics + (test_acc,))&#x27;&#x27;&#x27;    train_loss, train_acc = train_metrics # 返回训练集loss和精度    # 如果不符合要求 则报错并抛出后面的值    assert train_loss &lt; 0.5, train_loss    assert train_acc &lt;= 1 and train_acc &gt; 0.7, train_acc    assert test_acc &lt;= 1 and test_acc &gt; 0.7, test_acc\r\n","tags":["深度学习，线性回归"]},{"title":"共轭梯度法中参数alpha和beta的推导","url":"/2026/02/08/%E6%95%B0%E5%80%BC%E5%88%86%E6%9E%90/%E5%85%B1%E8%BD%AD%E6%A2%AF%E5%BA%A6%E6%B3%95%E4%B8%AD%E5%8F%82%E6%95%B0alpha%E5%92%8Cbeta%E7%9A%84%E6%8E%A8%E5%AF%BC/","content":"\r\n/* 强制让 MathJax 公式容器支持横向滚动 */\r\n.mjx-container, .MathJax_Display, .MathJax {\r\n    overflow-x: auto !important; /* 超出宽度时显示滚动条 */\r\n    overflow-y: hidden;          /* 隐藏垂直滚动条 */\r\n    max-width: 100%;             /* 限制最大宽度为屏幕宽度 */\r\n    -webkit-overflow-scrolling: touch; /* 优化移动端滑动体验 */\r\n}\r\n\r\n我们在这里推导pk = rk + βk − 1pk − 1中的参数α和β\r\nαk 的推导\r\n我们现在的目标是：在已知当前位置 xk\r\n和搜索方向 pk\r\n的情况下，确定我们要走多远 这意味着我们要选择一个 αk，使得函数\r\nφ\r\n在这条直线上达到最小值。 我们将 xk + 1 = xk + αpk\r\n代入 φ(u)\r\n的表达式中，把 φ 看作只关于\r\nα 的函数 f(α)：\r\nf(α) = φ(xk + αpk)\r\n展开 φ(u) = uTAu − 2uTb\r\nf(α) = (xk + αpk)TA(xk + αpk) − 2(xk + αpk)Tb\r\n展开括号（利用 A\r\n的对称性，xkTApk = pkTAxk）\r\n$$\r\nf(\\alpha) = \\underbrace{\\mathbf{x}_k^T \\mathbf{A} \\mathbf{x}_k -\r\n2\\mathbf{x}_k^T \\mathbf{b}}_{\\text{常数项}} + 2\\alpha \\mathbf{p}_k^T\r\n\\mathbf{A} \\mathbf{x}_k + \\alpha^2 \\mathbf{p}_k^T \\mathbf{A}\r\n\\mathbf{p}_k - 2\\alpha \\mathbf{p}_k^T \\mathbf{b}\r\n$$\r\n为了求最小值，我们对 α\r\n求导，并令导数为 0\r\n$$\r\n\\frac{d f(\\alpha)}{d \\alpha} = 2 \\mathbf{p}_k^T \\mathbf{A} \\mathbf{x}_k\r\n+ 2\\alpha \\mathbf{p}_k^T \\mathbf{A} \\mathbf{p}_k - 2 \\mathbf{p}_k^T\r\n\\mathbf{b} = 0\r\n$$\r\n整理\r\nα(pkTApk) + pkT(Axk − b) = 0\r\n因为 Axk − b = −rk，所以\r\nα(pkTApk) − pkTrk = 0\r\n最后解出 α\r\n$$\r\n\\alpha_k = \\frac{\\mathbf{p}_k^T \\mathbf{r}_k}{\\mathbf{p}_k^T \\mathbf{A}\r\n\\mathbf{p}_k}\r\n$$\r\n你会发现上面的公式分子是 pkTrk，但在最终算法里我们写的是\r\nrkTrk。为什么它们会相等？\r\n回顾 pk\r\n的定义：pk = rk + βk − 1pk − 1。\r\n两边同时左乘 rkT\r\n$$\r\n\\mathbf{r}_k^T \\mathbf{p}_k = \\mathbf{r}_k^T \\mathbf{r}_k + \\beta_{k-1}\r\n\\underbrace{\\mathbf{r}_k^T \\mathbf{p}_{k-1}}_{0}\r\n$$\r\n在共轭梯度法中，有一个重要的性质：当前的残差 rk\r\n与旧的搜索方向 pk − 1\r\n正交（即 rkTpk − 1 = 0）\r\n因此，分子可以简化为\r\npkTrk = rkTrk\r\n最终得到\r\n$$\r\n\\alpha_k = \\frac{\\mathbf{r}_k^T \\mathbf{r}_k}{\\mathbf{p}_k^T \\mathbf{A}\r\n\\mathbf{p}_k}\r\n$$\r\nβk 的推导\r\n我们在算法中定义了搜索方向的更新公式：\r\npk + 1 = rk + 1 + βkpk\r\n我们的目标是找到一个合适的 βk，使得新的方向\r\npk + 1\r\n与旧的方向 pk\r\n关于矩阵 A\r\n共轭 #### 列出共轭条件 根据共轭梯度的定义，我们需要\r\npk + 1TApk = 0\r\n将 pk + 1\r\n的定义代入上述条件中\r\n(rk + 1 + βkpk)TApk = 0\r\n展开括号\r\nrk + 1TApk + βkpkTApk = 0\r\n移项并求解 βk\r\n$$\r\n\\beta_k = - \\frac{\\mathbf{r}_{k+1}^T \\mathbf{A}\r\n\\mathbf{p}_k}{\\mathbf{p}_k^T \\mathbf{A} \\mathbf{p}_k}\r\n$$\r\n这个公式虽然正确，但计算起来很麻烦（需要做矩阵乘法 Apk）。我们可以利用\r\nαk\r\n的更新公式来化简它。 回顾残差的更新公式：rk + 1 = rk − αkApk。\r\n我们可以反解出 Apk：\r\n$$\r\n\\mathbf{A} \\mathbf{p}_k = -\\frac{1}{\\alpha_k} (\\mathbf{r}_{k+1} -\r\n\\mathbf{r}_k)\r\n$$\r\n将这个式子代入 βk 分子的 Apk\r\n中：\r\n$$\r\n\\text{分子} = \\mathbf{r}_{k+1}^T \\mathbf{A} \\mathbf{p}_k =\r\n\\mathbf{r}_{k+1}^T \\left[ -\\frac{1}{\\alpha_k} (\\mathbf{r}_{k+1} -\r\n\\mathbf{r}_k) \\right]\r\n$$\r\n$$\r\n= -\\frac{1}{\\alpha_k} (\\mathbf{r}_{k+1}^T \\mathbf{r}_{k+1} -\r\n\\underbrace{\\mathbf{r}_{k+1}^T \\mathbf{r}_k}_{0})\r\n$$\r\n又因为新残差与旧残差正交 (rk + 1Trk = 0)，所以分子简化为\r\n$$\r\n-\\frac{1}{\\alpha_k} \\mathbf{r}_{k+1}^T \\mathbf{r}_{k+1}\r\n$$现在看分母，回顾 $\\alpha_k =\r\n\\frac{\\mathbf{r}_k^T \\mathbf{r}_k}{\\mathbf{p}_k^T \\mathbf{A}\r\n\\mathbf{p}_k}$，所以分母其实等于：\r\n$$\r\n\\mathbf{p}_k^T \\mathbf{A} \\mathbf{p}_k = \\frac{\\mathbf{r}_k^T\r\n\\mathbf{r}_k}{\\alpha_k}\r\n$$ 化简后分子和分母相除\r\n$$\r\n\\beta_k = - \\frac{-\\frac{1}{\\alpha_k} \\mathbf{r}_{k+1}^T\r\n\\mathbf{r}_{k+1}}{\\frac{1}{\\alpha_k} \\mathbf{r}_k^T \\mathbf{r}_k}\r\n$$\r\n消去 $-\\frac{1}{\\alpha_k}$ 和 $\\frac{1}{\\alpha_k}$，最终就得到\r\n$$\r\n\\beta_k = \\frac{\\mathbf{r}_{k+1}^T \\mathbf{r}_{k+1}}{\\mathbf{r}_k^T\r\n\\mathbf{r}_k}\r\n$$\r\n","tags":["数值分析"]}]