---
title: CNN的设计原则与数学推导
Date: 2026-02-08 13:00:00
tags:
  - 卷积神经网络
mathjax: true
categories:
  - 深度学习
  - CNN
---
<style>
/* 强制让 MathJax 公式容器支持横向滚动 */
.mjx-container, .MathJax_Display, .MathJax {
    overflow-x: auto !important; /* 超出宽度时显示滚动条 */
    overflow-y: hidden;          /* 隐藏垂直滚动条 */
    max-width: 100%;             /* 限制最大宽度为屏幕宽度 */
    -webkit-overflow-scrolling: touch; /* 优化移动端滑动体验 */
}
</style>
### 原则一：平移不变性
假设我们需要检测图像中是否有“一只猫”。无论这只猫位于图像的左上角 $(x_1, y_1)$ 还是右下角 $(x_2, y_2)$，它都是猫。 在全连接网络中，识别左上角的权重 $w_{top\_left}$ 和识别右下角的权重 $w_{bottom\_right}$ 是完全独立的。这意味着模型必须在不同位置重新学习什么是“猫
我们采用**参数共享**的解决方案
我们不应该为图像的每个位置学习单独的检测器，而应该定义一个通用的检测器（卷积核、滤波器），让它在整个图像上**滑动**
- 如果在位置$\mathbf{A}$检测到了特征$f$，那么同样的检测器在位置$\mathbf{B}$也应该能检测到特征$f$
- 数学上，这意味着函数$f$ 对于输入 $x$ 的平移操作 $T$ 满足 $f(T(x)) = f(x)$

### 原则二：局部性定理
全连接试图试图建立所有像素间的联系，这是及其耗费内存的。因为图像像素具有很强的空间相关性，即像素 $x_{i,j}$ 的值与其邻域（如 $x_{i+1, j}$）高度相关，但与距离很远的像素（如 $x_{i+500, j+500}$）的相关性几乎为零
所以我们采取**稀疏连接**
我们限制神经元只与输入图像的一个小窗口（例如$3\times3$或者$5\times5$）相连，这个区域充分地减少了计算量

### 原则三：层级与感受野
虽然第一层只关注局部，但我们最终需要识别整张图像。局部的线条如何组合成形状，形状又如何组合成物体？
我们使用**感受野扩张**
- 第 1 层神经元看到 $3 \times 3$ 的像素
- 第 2 层神经元看到第 1 层的 $3 \times 3$ 个输出，这实际上覆盖了原始图像中更大的区域
- 随着层数加深，高层神经元能“看见”原始图像中更广阔的区域，从而捕获**全局语义信息**

## 数学形式化
我们如何从全连接公式变为卷积公式？
对于输入图像 $X$ (展开为一维向量) 和输出 $H$，全连接层定义为

$$
h_i = \sum_{j} W_{i,j} x_j
$$

其中 $W_{i,j}$ 表示第 $j$ 个输入像素对第 $i$ 个输出神经元的权重。这包含了所有可能的相互作用
根据**原则二**，神经元 $i$ 只应当受其附近的输入 $j$ 影响。如果 $|i - j| > \Delta$（$\Delta$ 为邻域范围），则强制 $W_{i,j} = 0$。公式变为

$$
h_i = \sum_{j=i-\Delta}^{i+\Delta} W_{i,j} x_j
$$

此时矩阵 $\mathbf{W}$ 变成了带状矩阵，这是一个稀疏矩阵
根据**原则一**，检测特征的权重不应依赖于具体位置 $i$，即 $W_{i, j}$ 不应该取决于 $i$，而只取决于输入和输出的**相对位置** $(i-j)$
我们将 $W_{i,j}$ 替换为 $w_{i-j}$（这里 $w$ 就是我们的卷积核）

$$
h_i = \sum_{j=i-\Delta}^{i+\Delta} w_{i-j} x_j
$$

如果我们令 $a = i-j$，则 $j = i-a$。通过变量代换，我们得到标准的**离散卷积**

$$
h_i = \sum_{a=-\Delta}^{\Delta} w_a x_{i-a}
$$

