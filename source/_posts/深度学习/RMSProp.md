---
title: RMSProp
Date: 2026-02-08 13:00:00
tags:
  - 深度学习
mathjax: true
---
<style>
/* 强制让 MathJax 公式容器支持横向滚动 */
.mjx-container, .MathJax_Display, .MathJax {
    overflow-x: auto !important; /* 超出宽度时显示滚动条 */
    overflow-y: hidden;          /* 隐藏垂直滚动条 */
    max-width: 100%;             /* 限制最大宽度为屏幕宽度 */
    -webkit-overflow-scrolling: touch; /* 优化移动端滑动体验 */
}
</style>

RMSProp 是一种 **自适应学习率**方法。它通过计算梯度的**二阶矩**的指数加权移动平均，来对每个参数的学习率进行独立缩放

$$
\begin{aligned}
E[\mathbf{g}^2]_t &= \beta E[\mathbf{g}^2]_{t-1} + (1-\beta) (\mathbf{g}_t \odot \mathbf{g}_t) \\
\mathbf{x}_{t+1} &= \mathbf{x}_t - \frac{\eta}{\sqrt{E[\mathbf{g}^2]_t + \epsilon}} \odot \mathbf{g}_t
\end{aligned}
$$

其中

- $\odot$ 表示哈达玛积，即逐元素相乘。
- $E[\mathbf{g}^2]_t$ 是梯度平方的累积量。
- $\beta$ 是**衰减率**（典型值为 0.9 或 0.99）。
- $\epsilon$ 是防止分母为零的**平滑项**。

### 构造与推导
我们从RMSProp的根源，**AdaGrad**开始讲起

动量法解决了全局震荡的问题，但是我们的所有参数共用一个学习率$\eta$
在处理稀疏特征或尺度差异巨大的参数（例如深度nn的网络）时，我们希望

- 对于频繁更新或梯度很大的参数，降低其学习率，防止其发散
- 对于频繁更新或梯度很大的参数，增大其学习率

为了实现上述目标，我们需要构造一个调节器，这个调节器应该与该方向上的历史梯度的幅度成反比
那么，我们该如何衡量一个参数在过去一段时间内的梯度**大小**？
简单的累加梯度$\sum g_t$肯定是不行的，因为正负梯度会互相抵消（实际上是剧烈震荡）
因此，我们必须累加梯度的**平方**
定义$r_t$为知道$t$时刻的所有梯度的平方和

$$
r_t=\sum_{\tau=1}^{t}g_\tau^2
$$

现在，我们使用$r_t$作为分母，对全局学习率进行归一化，这就是AdaGrad的核心公式

$$
w_{i+1}=w_i-\frac{\eta}{r_t+\epsilon}\odot g_t
$$

- $\epsilon$是一个防止分母为0的常数

#### AdaGrad的致命缺陷
虽然AdaGrad解决了“各向异性”问题，但是在深层神经网络学习的优化场景中，他暴露出了一个致命的问题
我们观察$r_t$的定义

$$
r_t = \sum_{\tau=1}^{t} g_\tau^2 = r_{t-1} + g_t^2
$$

由于$g_t^2$是非负的，我们会发现，整个式子是单调递增的，也就是说$r_t$是单调递增的！
随着训练迭代次数$t$趋近于$\infty$

- $r_t\to\infty$
- 有效学习率$\frac{\eta}{\sqrt{r_t+\epsilon}}\to0$

如果在找到最小值之前，学习率就已经减小到几乎为0，参数更新就会停止，无法到达最优解，这在深度神经网络中是不可接受的

所以我们引入**RMSProp**修正
我们需要一种方法，既能衡量梯度的大小，又只关注最近的历史，而不是从盘古开天辟地（$t=1$）开始累积
我们可以通过**指数加权移动平均**来实现。我们不再直接求和，而是使用一个**衰减系数**$\beta$

### RMSProp 的构造步骤
我们将 AdaGrad 的累积公式：


$$
r_t = r_{t-1} + g_t^2
$$

修改为 RMSProp 的加权更新公式：

$$
E[g^2]_t = \beta \cdot \underbrace{E[g^2]_{t-1}}_{\text{历史信息}} + (1-\beta) \cdot \underbrace{g_t^2}_{\text{当前信息}}
$$

**步骤解析**：
1. 每经过一步，旧的累积量 $E[g^2]_{t-1}$ 就会乘以 $\beta$（例如 0.9）。这意味着很久以前的梯度信息会以 $0.9, 0.81, 0.729...$ 的速度迅速衰减，不再主导分母
2. 当前的梯度平方 $g_t^2$ 被赋予权重 $(1-\beta)$ 加入累积量
3. 这种方式使得 $E[g^2]_t$ 不再是单调递增的，它变成了一个在该参数梯度平均强度附近的**滑动窗口估计量**
结合上述推导，我们得到了RMSProp的完整形式
对于参数 $\mathbf{w}$ 和梯度 $\mathbf{g}_t$
- 计算梯度平方的移动平均

$$
v_t = \beta v_{t-1} + (1-\beta) g_t^2
$$

这里常用符号 $v_t$ 或 $h_t$ 表示累积量 $E[g^2]_t$

- 参数更新

$$
w_{t+1} = w_t - \frac{\eta}{\sqrt{v_t + \epsilon}} g_t
$$

这就得到了我们最后的公式